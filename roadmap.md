Implementation Plan for an Ontology-Driven AI Tutor (Minimal POC)
1. Scope and Objectives
This plan outlines a minimal proof-of-concept (POC) for an ontology-driven AI tutor system in a STEM domain (Physics or Mathematics). The focus is on implementing core features with limited scope (e.g. a specific Physics topic like Newton’s laws or a Math topic like quadratic equations). The POC will demonstrate how an ontology-based domain model can be integrated with a Large Language Model (Anthropic’s Claude) and a conversational AI framework (DeepPavlov) to simulate a tutoring session. By limiting the content scope, we can concentrate on integrating the components and proving the concept, rather than covering an entire curriculum. Key objectives include:
Ontology-Driven Tutoring: Use a domain ontology to represent knowledge (concepts, relationships, prerequisites) in the chosen STEM subject. The ontology will guide the tutor’s dialog and content selection (ensuring the AI covers prerequisite concepts and uses correct domain terminology).
LLM Integration: Leverage Anthropic’s Claude to generate natural language explanations, answer questions, and engage in dialogue with the student. Claude will be prompted and constrained using information from the ontology (to maintain accuracy and relevance).
Conversational Interface: Utilize DeepPavlov as the framework for dialog management and to provide an avatar interface for the tutor. The avatar (a virtual tutor character) will interact with the user through text (and potentially speech) in a local environment.
Local Deployment: Implement and test the POC on a local machine (using the Cursor IDE for development). This ensures we can iterate quickly and maintain control of all components. (In the future, the architecture can be scaled to cloud deployment if needed.)
Demonstrate Knowledge Retention: Although a full user study is beyond a POC, we will set up mechanisms (e.g. quizzes, concept recall questions) to illustrate how the tutor could improve a learner’s knowledge retention compared to static learning. We’ll focus on how to measure retention qualitatively (via testing) in this POC.
By the end of the POC, we should have a working tutor bot that can teach/explain a few concepts in physics or math, ask the learner questions, and adapt the dialog based on an underlying ontology of the topic. This will validate the feasibility of the approach before scaling up.
2. Technology Stack and Tools
To build the POC, we will use a Python-based stack integrating the ontology, the LLM, and the conversational avatar. Key components and recommended tools are:
Ontology Management: We need to create and manage a domain ontology for Physics/Math. The ontology can be designed using a tool like Protégé (for visual editing), and then imported into Python. For Python integration, libraries such as Owlready2 or RDFlib are ideal. Owlready2 is a powerful choice since it allows importing OWL ontologies and manipulating classes/relationships as Python objects​
OWLREADY2.READTHEDOCS.IO
. It even includes an OWL reasoner (HermiT) for logical inference. Alternatively, RDFlib can be used to store and query the ontology (with SPARQL support) in Python​
RDFLIB.READTHEDOCS.IO
. These libraries will let the tutor query the ontology (e.g. find prerequisites of a concept, or retrieve the definition of a term) at runtime.
Large Language Model: We will integrate Anthropic’s Claude, a state-of-the-art LLM known for safe and coherent conversational abilities​
ANTHROPIC.COM
. Claude will handle the natural language generation for explanations, answers, and tutor dialog. We will access Claude via its API (using the official anthropic Python SDK or HTTP calls). The development environment (Cursor IDE) will be configured with the API key. Claude’s prompts will be dynamically constructed by the system – for example, a system prompt might include relevant facts from the ontology or pedagogical instructions (“explain in a simple way, ask a follow-up question if the student seems confused,” etc.). Claude has a large context window, allowing us to include ontology-derived context in each query. (Note: The POC will use the cloud API for Claude, while other components run locally. This is acceptable since the heavy model inference is offloaded to Anthropic’s service.)
Conversational AI Framework (Avatar): We use DeepPavlov, an open-source framework for building chatbots and virtual assistants​
DEEPPAVLOV.AI
. DeepPavlov will serve as the dialog manager and avatar interface. Specifically, we can leverage DeepPavlov Agent to orchestrate the conversation flow and integrate multiple skills via API​
DEEPPAVLOV.AI
. In this POC, the main “skill” is calling the Claude LLM with appropriate prompts, but we may also include a Q&A retrieval skill from the ontology (for example, to fetch a definition or a formula). DeepPavlov provides tools to manage multi-turn dialogues, track context, and even handle simple NLU (though Claude can handle understanding of input, we might still use DeepPavlov for intent classification if needed – e.g. detect if the user asked a factual question vs. needs a hint). The avatar interface can be a simple GUI or web interface where a character (avatar) represents the tutor. DeepPavlov doesn’t come with a 3D avatar by default, but it supports integration with messaging platforms and web UIs. For the POC, we can use a basic HTML front-end (or DeepPavlov’s Telegram interface, etc.) to display the tutor’s responses. Optionally, text-to-speech (TTS) can be added for the avatar to “speak,” and a simple animated avatar image can be shown alongside the text. The focus, however, is on the dialog logic, not the graphics – so a text-based chat UI is sufficient initially.
Development Environment: All development will be done in Cursor IDE, ensuring a streamlined coding experience with built-in AI code assistants. Cursor IDE will help in writing and refactoring code (especially prompt engineering for Claude and integration code for DeepPavlov). The project will be organized into modules (ontology handling, API calls, dialog management) for clarity. Version control (Git) should be used to track changes. The environment being local ensures we can run and test quickly.
Deployment: The POC will run locally on a standard PC. We will run the DeepPavlov agent and any web interface on localhost. The ontology can be stored as a local OWL/RDF file loaded by Owlready2 or RDFlib. Claude’s API calls will require internet access (API calls to Anthropic’s servers). No other cloud services are needed for the POC. After successful local tests, we will document how to deploy the system to a cloud server or VM if needed (ensuring the architecture is cloud-ready, e.g. using Docker for the DeepPavlov service, etc., but that’s optional future work).
Summary of Python Libraries: owlready2 (or rdflib) for ontology, anthropic SDK (or requests for API calls) for Claude, deeppavlov for the conversational agent, plus standard libraries (json, logging, etc.). These should be installed in the local environment. (Note: DeepPavlov may have its own installation and configuration steps to load the dialogue agent and any pre-trained models needed for NLU modules.)
3. Data & Knowledge Sources (Physics/Math Domain)
A critical part of an ontology-driven tutor is the knowledge base it relies on. For this POC, we will incorporate or reference existing datasets/ontologies to avoid reinventing domain knowledge from scratch. Below are the top 3 recommended sources for Physics/Mathematics education content:
Wikidata / DBpedia (Physics/Math Concepts): Wikidata is a huge collaborative knowledge graph that includes factual data on scientific concepts, formulas, and relationships. For example, Wikidata has entries for physics concepts (like force, energy, Newton’s laws) with properties such as units, part-of, instance-of etc. It can be queried via SPARQL or their API. Using Wikidata, we can retrieve definitions or related entities to enrich the tutor’s responses. DBpedia similarly provides structured data extracted from Wikipedia. These sources are not education-specific, but they cover most physics and math topics and can serve as a backbone for factual information. DeepPavlov’s platform even has built-in support for querying Wikidata and custom knowledge graphs​
ACLANTHOLOGY.ORG
, which indicates it’s feasible to integrate these sources. For the POC, we might use a handful of queries to Wikidata (e.g. to get the formula for Newton’s second law or the Wikipedia abstract of a concept) and feed that into Claude as context when needed.
Open Educational Resources / Textbook Ontologies: There are open datasets from educational content providers that we can leverage. One example is OpenStax (open textbooks for Physics and Math). While OpenStax content is in textbook form, we could extract a few key sections (and even structure them as an ontology of topics and subtopics). Another example is the Khan Academy knowledge structure – Khan Academy’s topics and prerequisites form a graph that could act as an ontology of how concepts relate (though direct API access is limited, Khan provides data dumps for exercises, and their topic tree is publicly visible). Additionally, research exists on converting textbooks to ontologies; for instance, one approach automatically built a High School Physics ontology from textbook content​
RESEARCHGATE.NET
. For our POC, we can manually create a small ontology of the chosen domain topic. For example, if focusing on Newtonian mechanics, define classes like Concept (with instances: Force, Mass, Acceleration, NewtonsSecondLaw) and relationships like hasPrerequisite (e.g. NewtonsSecondLaw hasPrerequisite Force,Mass,Acceleration), or hasFormula linking NewtonsSecondLaw to the formula F=ma. We can populate this mini-ontology with a dozen key concepts. To ensure accuracy, we draw data from OpenStax Physics or a similar reputable source (which provides definitions and explanations we can include as annotations in the ontology). The goal is to reuse known correct content rather than rely solely on the LLM’s training data. This also ensures the tutor’s knowledge base is transparent and modifiable.
Domain-Specific Knowledge Graphs / Common Knowledge Bases: Besides educational texts, we can leverage existing knowledge graphs and ontologies in STEM. One notable resource is ConceptNet, a broad semantic network with millions of assertions of common sense and basic world knowledge​
MEDIA.MIT.EDU
. ConceptNet includes relations that might help an AI tutor – for example, it knows that “a derivative is a type of calculus concept” or “force is related to mass and acceleration”. While ConceptNet is not specifically an educational curriculum, it can supplement the ontology with general relationships (like analogy or commonsense facts that make explanations richer). Another potential resource is the Science Knowledge Graph Ontologies (SKGO)​
GITHUB.COM
 or the Ontology of Physics (e.g., the Ontology of Physics for Biology (OPB), which is a formal ontology of classical physics quantities​
SITES.GOOGLE.COM
). For simplicity, SKGO/OPB might be too large; however, we could extract from them a subset relevant to our scope (like units and physical constants relationships if needed). In the math domain, an example is OpenMath content dictionaries, which formally describe mathematical concepts (e.g., definitions of calculus concepts in a structured way). OpenMath has an RDF/OWL representation​
OPENMATH.ORG
 that could be imported for a math tutor POC. This would provide an ontology of mathematical objects (numbers, operations, etc.) if our POC needed that level of detail.
For the POC, we will pick one domain (Physics or Math) and accordingly choose the sources. A likely approach is: use a core manually-curated ontology (small, focused) and enrich it with data from Wikidata/DBpedia or an open textbook. For example, if the POC is Physics (Newton’s laws), we create an ontology with classes like PhysicalQuantity (Force, Mass, Acceleration as instances) and Law (NewtonSecondLaw instance, linking to those quantities). We then attach to each concept some metadata: e.g., a short description from Wikipedia/OpenStax as an annotation property, the formula, etc. This ontology (saved as tutor_domain.owl) will be loaded into our system. Additionally, we prepare a small set of Q&A pairs or examples from existing datasets (perhaps a couple of physics exercises from an open dataset) to test the tutor. Rationale: Using established datasets and ontologies accelerates development and ensures our tutor’s knowledge is reliable. It also demonstrates how the system could plug into larger knowledge sources in the future (for a full product, one might integrate a comprehensive knowledge graph of the entire high school physics curriculum). For now, the top 3 sources above provide content and structure that we will incorporate on a smaller scale.
4. System Architecture
The AI tutor system will be designed following a modular architecture, drawing from best practices of Intelligent Tutoring Systems (ITS). A classic ITS has a four-component architecture: Domain Model, Student Model, Tutoring Model, and Interface​
EN.WIKIPEDIA.ORG
. Our POC will primarily implement the Domain Model (ontology) and the Tutoring Model (AI logic using Claude), along with an Interface (avatar via DeepPavlov). The Student Model (tracking the learner’s knowledge state) will be rudimentary in the POC – we may simulate it with simple variables (e.g., which concepts the student has seen or answered correctly). Architecture Overview:


Figure: High-level architecture of the ontology-driven AI tutor. The system consists of the following components:
Domain Ontology & Knowledge Base: The structured representation of knowledge (concepts, facts, relationships) in the chosen domain. This is stored in an OWL/RDF file and loaded via Owlready2. The ontology can be queried for information like: “what are the prerequisites of this concept?”, “is this question related to Concept X?”, or “give the definition of Concept Y”. For example, if a student asks a question about “force”, the system can look up the ontology to find that Force is a PhysicalQuantity, measured in Newtons, and perhaps retrieve any definition stored. The ontology acts as the tutor’s expert domain model​
ITC.SCIX.NET
, ensuring the AI’s responses are grounded in a formal representation of the subject matter.
LLM Module (Claude API): This is the natural language generation and understanding engine. Rather than building a dialogue system from scratch, we delegate most language understanding and generation to Claude. Claude receives prompts that include the conversation context (previous dialogue turns) and any ontology-derived context. For instance, if the tutor wants Claude to explain Newton’s second law, it might construct a prompt: “The student is asking about Newton’s Second Law. The law states: Force = mass * acceleration. In simple terms, how would you explain this law to a student? Also, ask a question to check understanding.” Claude’s response is the tutor’s utterance to the student. Claude can also be used to interpret student responses or questions (e.g., if a student gives an answer, we can ask Claude to analyze if it’s correct or if there’s a misconception). We will use Claude in a steerable manner – for example, providing it with a system instruction to always use the ontology for factual info and to behave pedagogically (patient, encouraging). Anthropic Claude is designed to be helpful and harmless​
ANTHROPIC.COM
, which suits the tutoring use-case.
Dialogue Manager (DeepPavlov Agent): This component orchestrates the flow of the conversation. It decides when to call the LLM, when to fetch info from the ontology, and how to respond to the user. DeepPavlov Agent can manage multi-skill conversations​
DEEPPAVLOV.AI
, but in our POC we primarily have two “skills”: (1) the Tutoring skill (which uses Claude to generate answers/explanations) and (2) the Knowledge-query skill (which queries the ontology or other knowledge base for facts). The Dialogue Manager will handle the state of the conversation – for example, knowing which concept is being taught currently, whether the user answered the last question correctly, etc. Based on this state, it can decide the next action: e.g., if the user’s answer was wrong, the manager might retrieve a hint from the ontology (like a prerequisite concept definition) and ask Claude to use that in a follow-up explanation. If the user asked a direct factual question (“What is the unit of force?”), the manager can fetch that from the ontology (ontology says unit of Force is Newton) and then prompt Claude to formulate a helpful answer using that fact. Essentially, DeepPavlov will serve as the middleware that connects user input to the appropriate backend (Claude, ontology, or possibly a simple rule-based response for certain intents). It also logs interactions for analysis.
Avatar Interface: This is the user-facing interface. In the POC, it could be a simple console or chat window, but ideally we demonstrate an “avatar.” Using DeepPavlov’s capabilities, we can create a rudimentary web UI where a cartoon tutor or just a profile picture is shown, along with a chat bubble of the conversation. The student types questions or answers, and the tutor (Claude via our system) responds. If possible, we can integrate text-to-speech so the tutor’s responses are spoken (for immersion), and speech-to-text if we want voice input. However, to keep it minimal, a text chat interface suffices. The key is that this interface is local, meaning the student interacts with the tutor on the developer’s machine or a local server – all data stays local except calls to Claude’s API. The interface will display things like “Tutor is typing…” while Claude’s response is being generated, and then show the answer. DeepPavlov supports connecting to various frontends (e.g., a Telegram bot or a web chatbot UI), so we’ll choose one for demonstration.
Student Model & Session Data (POC-level): We will maintain a lightweight representation of the student’s progress. For example, a dictionary of concepts with flags for “taught” or “mastered” can be used. Each time the tutor covers a concept or the student answers a related question correctly, we mark it. This allows the system to avoid repeating content the student already knows and to perhaps give a summary at the end. The student model is simplistic in the POC, but it sets the stage for more complex tracking (like mastery probabilities) in a full system.
The architecture emphasizes loose coupling: the ontology is separate from the LLM (they communicate via queries and prompt injection), and the dialogue manager mediates everything. This makes the system maintainable – one can update the ontology without retraining models, or swap out Claude for another LLM if needed in future, etc. It also aligns with the idea of an ontology-based tutor where the AI’s knowledge can be audited and updated easily (a critical best practice in AI education systems).
5. Step-by-Step Implementation Guide
This section provides a step-by-step walkthrough to build the POC system, from setting up the ontology to integrating the AI and testing the tutor. Each step corresponds to an incremental build stage, allowing testing at that stage before moving on.
Step 1: Ontology Creation and Setup
1.1 Define the Domain Scope: Choose the specific topic in Physics or Math for the POC. For example, if Physics, we might pick “Newton’s Laws of Motion”. If Math, perhaps “Quadratic Equations”. Narrow down to a set of ~10-15 key concepts that are inter-related (for Newton’s laws: concepts could be Force, Mass, Acceleration, Newton’s Second Law, Unit (Newton), etc.; for Quadratics: Polynomial, Quadratic Equation, Parabola, Vertex, etc.). 1.2 Design the Ontology: Use Protégé (a free ontology editor) to create an ontology file (OWL format). Create classes for high-level types (e.g., Concept, Law, Quantity, Formula). Then create individuals or subclasses for each specific concept. Define relationships:
Hierarchical: e.g., subClassOf or an ontology taxonomy if needed (maybe not heavily needed for a small domain, but for math you might have “Quadratic Equation” subclass of “Equation”).
Pedagogical relations: prerequisite (X hasPrerequisite Y if concept Y should be understood before learning X). This is very important for a tutor. We manually decide these relations based on domain logic. (E.g., “Newton’s Second Law” hasPrerequisite “Force” and “Mass” and “Acceleration”; “Force” hasPrerequisite “Acceleration” (assuming they should know what acceleration is first), etc.).
Definition or description: We can use rdfs:comment or a custom property like hasDefinition to attach a short text definition to each concept. These definitions can be taken from a trusted source (like a textbook or Wikipedia).
Other properties as needed: For a physics example, we might add hasFormula linking a Law to an equation string, or hasUnit linking a quantity to its unit. For a math example, maybe link a formula to a graphical interpretation, etc. Keep it simple – a handful of properties.
1.3 Populate the Ontology: Fill in the individuals and relationships. Ensure the ontology is self-consistent (Protégé’s reasoner can help catch issues). For the POC, we aim for maybe ~10-20 entities and 10-15 relations between them. This is enough to allow meaningful queries. Save the ontology as domain_tutor.owl. 1.4 Install and Test Ontology Library: In the Python environment (Cursor IDE), install Owlready2​
OWLREADY2.READTHEDOCS.IO
. Load the ontology file in a small test script to verify it parses correctly. For example:
python
Copy
Edit
from owlready2 import get_ontology
onto = get_ontology("file://path/to/domain_tutor.owl").load()
for cls in onto.classes(): 
    print(cls) 
This should list the classes. Also test a simple query: e.g., retrieve all prerequisites of a certain concept. If using Owlready2, you can do something like:
python
Copy
Edit
concept = onto.search_one(iri="*NewtonSecondLaw")
print(list(concept.hasPrerequisite))
This should output the linked concepts. Getting these basic ontology operations working early is important. (If Owlready2 proves complex, RDFlib could be used: load the ontology as an RDF graph and run a SPARQL query for prerequisites. But Owlready2’s Pythonic access is convenient for a POC.) 1.5 (Optional) Reasoning: For advanced behavior, run the reasoner to compute any inferred relationships. Not critical for a small manually-built ontology, but if we set up class hierarchies or restrictions (e.g., a rule that all Laws require at least one prerequisite Quantity), the reasoner can check consistency. Owlready2 can invoke HermiT reasoner if needed​
OWLREADY2.READTHEDOCS.IO
. By the end of Step 1, we have a domain ontology accessible in Python. This ontology will act as the system’s knowledge graph.
Step 2: Integrating the LLM (Claude API)
2.1 Obtain API Access: Ensure we have API access to Anthropic’s Claude. Sign up for an API key if not already done. Install the Anthropic SDK (pip install anthropic in the environment) or prepare to use direct HTTP calls. In Cursor IDE, securely store the API key (perhaps in an environment variable or config file, not hard-coded). 2.2 Test Basic Prompting: Write a small Python function to query Claude. For example, using the SDK:
python
Copy
Edit
from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT
client = Anthropic(api_key="...")  
def ask_claude(prompt_text):
    resp = client.completions.create(
        model="claude-2", 
        max_tokens_to_sample=300,
        prompt= prompt_text
    )
    return resp.completion
As a quick test, call ask_claude(HUMAN_PROMPT + "Hello, Claude" + AI_PROMPT) and see if we get a reasonable greeting response. This verifies that API calls work from our environment. We should also test multi-turn format (Claude uses a specific prompt format with HUMAN_PROMPT and AI_PROMPT tokens for conversation turns, the SDK handles some of this). 2.3 Design System Prompts: One of the key tasks is prompt engineering so that Claude acts like a tutor. We will create a system prompt or instruction that we prepend to every conversation. Claude (like other LLMs) can take instructions like “You are a helpful physics tutor. You will be provided with some domain knowledge and the student’s question. Use the knowledge to explain the concept step by step. Be Socratic: ask the student questions to keep them engaged. Ensure your answers are correct and based on the provided facts. If you don’t know, say you will think together,” etc. We craft this prompt to imbue the tutoring style and also to caution Claude to use only information from either the ontology or general knowledge (and not to fabricate formulas, for example). Anthropic’s Claude is known to follow instructions about being truthful and harmless​
ANTHROPIC.COM
, so we leverage that by explicitly telling it to double-check answers with provided data. For instance:
pgsql
Copy
Edit
SYSTEM: You are an AI Tutor helping a student learn physics. You have access to a knowledge base of physics facts. Always use the knowledge base information when relevant, and prefer to explain concepts at a high-school level. Do not just give the answer; guide the student. If the student makes an error, kindly correct them. If a question is unclear, ask for clarification.
(Claude’s API might not have a separate “system” role like OpenAI’s ChatGPT, but we can embed this instruction at the start of the prompt we send, before the human message.) 2.4 Connect Ontology to Prompts: Develop a strategy for using ontology data in prompts. For example, if the student asks a question, the Dialogue Manager (in Step 3) might decide to retrieve a relevant ontology snippet to help Claude. We can format the prompt as:
sql
Copy
Edit
<HUMAN>: [Ontology]
Newton’s Second Law – Definition: "The net force on an object is equal to the mass of the object multiplied by its acceleration (F = m * a)." Prerequisites: Force, Mass, Acceleration.

Student: "I don’t understand how Newton’s second law works in real life." 

<HUMAN>: Using the above knowledge, explain Newton’s second law with an everyday example.
<AI>:
Here we insert a brief chunk from the ontology (“Newton’s Second Law – Definition: … Prerequisites: …”) ahead of the user’s actual question in the prompt. This gives Claude immediate factual context to use. We will implement a small helper that given a topic or user query, finds relevant ontology entries (e.g., if query contains “Newton’s second law”, fetch that individual and its properties) and formats a knowledge context section. This approach is akin to retrieval-augmented generation (RAG), where the ontology plays the role of the retrieved knowledge. By connecting ontology data with LLM prompts in this way, we aim to reduce hallucinations and improve accuracy. 2.5 Test LLM with Domain Knowledge: Before integrating into the full system, test Claude with a few handcrafted prompts and ontology info. For example, simulate a Q&A:
Prompt Claude with a definition of a concept and ask it to explain or generate a question. Check that it uses the definition correctly.
Prompt Claude with a prerequisite relationship and ask it to formulate a hint (e.g., “The student doesn’t recall Concept Y which is a prerequisite for X, provide a hint about Y without giving it away”).
These tests ensure our prompt patterns yield the desired style of answer. We may refine wording or how we inject ontology info based on these trials. By the end of Step 2, we have the LLM component working: we can send it input (with ontology context) and get a pedagogically-sound response.
Step 3: Building the Conversational Pipeline with DeepPavlov
3.1 Install DeepPavlov: Install the DeepPavlov library (pip install deeppavlov) and ensure it’s working. DeepPavlov often uses config files (JSON configs) to define models and pipelines. However, for a custom pipeline, we might directly use the Python API. We’ll likely use DeepPavlov Agent, which allows integration of multiple skills via an API call​
DEEPPAVLOV.AI
. Check DeepPavlov’s documentation for a simple Echo bot or FAQ bot example to use as a template. 3.2 Plan Dialogue States: Although Claude can handle open conversation, we want some control. We outline a simple state machine or flow: e.g.,
State “Greeting”: Tutor greets the student and asks what they want to learn, or offers a first topic.
State “Teaching [Concept]”: Tutor explains a concept (using Claude) and perhaps asks a question to the student.
State “Awaiting Answer”: Student answers a quiz/question.
State “Feedback”: Tutor evaluates answer (with Claude’s help) and decides whether to elaborate more, correct the student, or proceed.
Next concept or wrap up.
In DeepPavlov, we might implement this with a Dialog Flow or just code. DeepPavlov’s Dialog Flow Framework (DFF) could be useful to script a sequence of turns, but to keep it minimal, we can manage state in Python code and call the necessary functions. 3.3 Integrate Skills: We create two main functions:
generate_tutor_response(user_input) – which takes the user’s last message and returns the tutor’s reply. Internally, this will use the ontology + Claude prompt logic from Step 2.
check_answer(user_input, expected_answer) – optionally, a function to judge a student’s answer to a quiz. This could use Claude as well (e.g., prompt Claude with the expected answer and the student’s answer and ask if it’s correct, or simpler: if it’s a numeric or multiple-choice answer, we can just compare strings).
Using DeepPavlov Agent’s mechanism, we can register a skill for any user input that is not part of a pre-defined scenario. For instance, treat everything the user says as something to feed into generate_tutor_response. The generate function will need to know the context/state: are we expecting an answer or is this a new question from student? We can keep a variable awaiting_answer = False and last_question = None in our code. If awaiting_answer is True, then the user_input is presumably an attempt to answer the tutor’s question. In that case, we call check_answer(user_input, last_question.expected) and formulate feedback. If awaiting_answer is False, then the user_input might be a question or statement of curiosity, so we use generate_tutor_response normally to address it. 3.4 Use DeepPavlov Agent: If using DeepPavlov Agent API, we would define a skill that on each input calls our logic. Pseudocode for agent setup:
python
Copy
Edit
from deeppavlov import Agent
tutor_agent = Agent(skills=[MyTutorSkill()], 
                    skills_selector=... )
tutor_agent.start_chat()
However, integrating with Claude might require asynchronous calls (Claude API might take a couple seconds). DeepPavlov can handle async by design since Agent is asynchronous. We might implement MyTutorSkill as an object with a method __call__(utterance, context, state) that returns a response. The context/state can store our awaiting_answer flag and which concept we’re on. Alternatively, we can bypass DeepPavlov’s complexity by managing the loop ourselves: in a simple while loop, read user input, call our response generator, and print the answer. This is simpler for a POC. But since the requirement is to use DeepPavlov, we try to at least use it to structure the project. Perhaps use DeepPavlov’s simple_server or telemetry to connect to a UI. 3.5 Avatar/Interface Integration: For the avatar interface, one easy approach is to use DeepPavlov’s REST API mode. We can wrap the Agent in a REST service (DeepPavlov provides a way to run an agent and get an API endpoint). Then we can create a small web page (HTML/JS) that continuously sends user messages to this API and displays responses (this could be done with a simple Flask app or even just an HTML + JavaScript calling the API). DeepPavlov might have a built-in chat UI if using their demo framework. If time is short, we can simply run the conversation in the console. But to fulfill the “avatar” aspect, we could show an avatar image. For instance, an ASCII art or a simple image in the console might not be possible, so a minimal GUI might be better:
Option A: Use a Jupyter notebook interface (if running in Jupyter) with display() of an image and text. But that’s clunky for real conversation.
Option B: Write a minimal Flask app that serves a webpage with a chat interface. The chat interface can be something like: a fixed avatar picture on top, a scroll box for messages, and an input box. On submit, it calls our backend (which calls Claude and gets reply).
Option C: Use an existing messenger integration. DeepPavlov can integrate with Telegram. Setting up a Telegram bot for the tutor might actually be quick: you create a bot with BotFather, get the token, and use DeepPavlov’s telegram connector to pipe messages to our skill. Then the avatar is basically the bot’s profile picture. This avoids building a UI from scratch. For POC demonstration, interacting via Telegram (locally) could suffice and looks “avatar-like”.
Given local deployment, we might prefer the local web UI approach to avoid external dependencies. We can choose a simple library like Gradio or Streamlit for a quick chat UI if allowed (those would open a local browser UI). However, to keep within our stack, a lightweight Flask + vanilla JS is fine. 3.6 Bring It All Together: Now, integrate: When the user sends a message, DeepPavlov (or our loop) will:
If it’s a start of conversation or a general query: find if it matches any known intent (maybe check if the user said “help” or “menu” to list topics – we can have a simple rule).
Otherwise, determine if we were expecting an answer. Use the awaiting_answer logic.
If expecting an answer: call check_answer. If correct, reply with praise and either move to next subtopic or ask if they want to try a harder question. If incorrect, perhaps use Claude to gently correct: e.g., prompt: “The student answered X but the correct answer was Y (from ontology). Provide a hint or explanation that clarifies the misunderstanding.” Then mark awaiting_answer = False (or maybe we allow retry).
If not expecting an answer: This is a question or new request from the student. Determine the topic. Possibly use a simple classifier or keyword match with ontology terms. For example, if user says “Tell me about force”, we identify “force” as a concept in ontology. If we have that concept, we might set the current topic to that, retrieve its info, and ask Claude to explain it. If the user says something broad like “I don’t understand physics”, we could either ask them to be more specific or pick a default topic. For POC, we can assume the user will inquire about the chosen domain (we won’t handle completely unrelated queries).
Formulate the prompt for Claude using the methods from Step 2 (inject relevant ontology info, etc.).
Get Claude’s response and send it back to the user. If the Claude’s response includes a question for the student (we can design it to do so when appropriate), then set awaiting_answer = True and store the expected answer if we know it. (How to get expected answer? If we told Claude to ask a specific known question, we know it; if Claude spontaneously asks something, we might not. To simplify, we might script the questions ourselves: e.g., after explaining a concept, we deliberately ask a known question from our ontology data. For example, after explaining Newton’s second law, we ask the student: “If you double the mass while keeping acceleration same, what happens to force?” We know the answer (“it doubles”). Claude can be used to present the question, but we, as the system, know the answer to evaluate later. This way we aren’t trying to parse Claude’s question.)
3.7 Logging and Debugging: Throughout development, enable logging of each interaction and intermediate steps (which ontology concepts were fetched, what prompt was sent to Claude, what Claude replied, etc.). This will help debug if responses are off. Test the conversation flow with a few scenarios: e.g., user asks an open question, user follows the tutor’s prompts, user gives a wrong answer, etc., and ensure the logic holds.
Step 4: Predefined Test Cases for Demonstration
To validate the POC, we will run predefined test conversations that cover key functionalities. These test cases serve as demos to stakeholders and also act as regression tests for the system as we tweak it. We will script the following scenarios:
Test Case 1: Straight Q&A – The user asks a direct question on a concept. Example: User: “What is Newton’s second law?” Expected behavior: The tutor (Claude with ontology help) provides the definition and explanation of Newton’s second law, possibly an example, and then (optionally) asks the user a follow-up question to check understanding. We verify the response is accurate (uses the ontology fact F = m*a correctly) and pedagogically sound. We also check that if the user doesn’t respond further, the tutor might gently prompt or offer more help.
Test Case 2: Step-by-Step Teaching – The user is new to the topic and says something general like “I want to learn about Newton’s laws.” The tutor should start by introducing the first law or an overview, then move through the concepts. It might say: “Sure! Newton’s laws consist of three fundamental principles. Let’s start with the first law (inertia)... [explanation]. Do you understand, or shall I explain further?” We then simulate the user maybe asking for clarification or just saying “ok”. The tutor then proceeds to second law, explains, and asks a question. We simulate the user giving a correct answer. The tutor gives positive feedback and moves on. Finally, after third law, the tutor might quiz the user on something spanning all three. This test case ensures the ontology’s prerequisite structure is used (it followed the sequence 1->2->3 because that’s logical, and maybe the ontology was set to indicate that order or we encoded it in script).
Test Case 3: Handling Wrong Answer – We simulate a scenario where the tutor asks, “If we double the mass in Newton’s second law scenario, what happens to force?” (expected answer: doubles), and the user responds incorrectly (“It stays the same.”). The system should detect this is wrong. The tutor (Claude) should then respond with something like, “Not quite. Remember the formula F = m * a – if m increases, F will increase too. So doubling the mass would double the force (assuming acceleration is constant). Let’s go over that again…” The important part is that the tutor doesn’t just say “Wrong.” It gives a tailored explanation. We check that Claude’s feedback used the ontology knowledge (it knew the correct relationship). This test ensures the check_answer logic and Claude prompt for feedback are working.
Test Case 4: Out-of-scope or Idle Chat – Test how the system behaves with non-domain input. For example, user asks a completely unrelated question (“Who won the World Cup in 2018?”). Our tutor should politely say it’s out of scope or redirect back to physics. We might implement a simple check: if a question doesn’t match any domain keywords and ontology has nothing relevant, Claude could answer generally. But for safety, probably better to respond: “I’m not sure about that – let’s stick to physics for now. Did you have a physics question?” This keeps the tutor focused. This test is more about ensuring the system is robust and doesn’t break on odd input.
Test Case 5: End of Session – Have the user or tutor initiate ending the session (“That’s all for now, thanks.”). The tutor should say a friendly goodbye, perhaps a summary: “Great job today! We learned about X, Y, and Z. Keep practicing, and see you next time!” We ensure the conversation closes gracefully.
We will run these test cases in the local environment (possibly by simulating user input sequentially) and document the dialogues. These will demonstrate the POC functionality to any evaluators.
Step 5: Evaluation of Knowledge Retention (POC Focus)
Although doing a full evaluation requires actual learners, we can simulate some metrics in the POC or outline how we would evaluate knowledge retention. For the POC implementation:
We incorporate quizzes or recall questions after a concept is taught. The tutor’s design (as above) already includes asking questions and checking answers. We can measure whether the student (even if simulated) gets them right on first try or after hints. For demonstration, we could have a fake user profile that “improves” after the tutoring (e.g., first the user gets questions wrong, then after tutoring, the user gets similar questions right). This would illustrate improved retention/understanding.
We record metrics like:
Quiz Score: How many of the tutor’s questions were answered correctly (perhaps after the session, the tutor can report “You answered 3 out of 4 questions correctly after our session”).
Mistake Correction: If the student made an error, did they get it right upon a second attempt after the tutor’s explanation? This indicates successful remediation.
Retention Simulation: If we want to simulate retention, we could ask a question at the beginning and the same question at the end to see improvement. For example, ask “What is force?” at the very start (user might not know or give a flawed answer), then teach everything, then ask again at the end. If the final answer is more complete, that’s a qualitative measure of knowledge gain. We can demonstrate this in the POC by scripting such a pre/post question.
Though these are not real user measurements, they show the concept of evaluation. In documentation, we will mention how actual user studies could be done: e.g., have a group of students use the AI tutor and a control group study the same material via textbook, then give both groups the same test after a week and compare scores to see if the AI tutor group retained more information. For now, we ensure the POC can log data needed for such metrics (like timestamps of interactions, correctness of answers, etc., to compute retention metrics later). At this stage, we have a fully integrated system that we can run and interact with. We should perform multiple ad-hoc conversations to fine-tune prompts or fix any issues.
6. Evaluation Metrics – Focus on Knowledge Retention
Since this is a POC, formal evaluation will be limited, but we target knowledge retention as the key metric to improve. Knowledge retention refers to the student’s ability to remember and recall information over time after using the tutor. To evaluate if our AI tutor improves retention compared to a baseline (like reading a textbook), we propose:
Immediate Post-Session Quiz: After the tutoring session, the student should take a short quiz on the material. In the POC, we can simulate or have the tutor itself administer this quiz (e.g., a set of 3 questions covering the taught concepts). The score of this quiz indicates how well the student understood the material right after learning. High scores would suggest good short-term retention.
Delayed Recall Test: More importantly, retention implies memory over time. For a real evaluation, we’d test the student a few days or a week later on the same concepts. In the POC context, we cannot actually wait, but we can simulate this by incorporating a second round of questions on the same content after some intervening dialogue. For instance, the tutor could say “Let’s review: do you recall what Newton’s second law states?” at the end of the session, which is a kind of recap. If the student (or simulated student) can answer correctly without help, that’s a positive sign. In a real scenario, we would compare this with how a student might forget if they only read once without practice.
Comparative Retention (AI Tutor vs. Standard): To truly measure improvement, we’d have a control method. For POC demonstration, one way is to use the AI tutor itself as both teacher and baseline. For example, show that if the tutor simply stated the facts once and didn’t engage the student (simulate a “lecture”), the student’s retention is lower than if the tutor engaged with questions and answers. We can illustrate this by scenario: Tutor explains a concept without interaction, then student is quizzed (perhaps they score lower) versus tutor explains and then engages the student in practice questions, then student is quizzed (likely scores higher because practice improved retention). This aligns with educational research that active recall practice strengthens memory. Our AI tutor design naturally incorporates quizzes and active recall, which should improve knowledge retention.
Qualitative Feedback: If we had real users, we’d also gather feedback on how confident they feel about the material after using the tutor vs before. For the POC, we can just note that the conversational approach is intended to keep the student more engaged (and engagement often correlates with better retention).
In terms of metrics we can log:
The number of times the student needed help on the same concept. (If our system finds it has to re-explain something multiple times, that concept might not be retained well.)
Whether the student can answer a question about a concept at the end that they got wrong initially. This improvement can be counted as a success instance.
Perhaps measure coverage vs mastery: out of the concepts in the ontology that were targeted, how many does the student show mastery of by the end (through correct answers or confident explanations).
For the POC demonstration, we will emphasize the approach to measuring retention: e.g., use tests and quizzes integrated into the tutor​
CELEBALTECH.COM
. The tutor can “measure knowledge retention through tests and quizzes” as an integrated part of the learning process​
CELEBALTECH.COM
. We will show an example of the tutor giving a quiz and then saying something like “You initially struggled with concept X but after our discussion you got it right – this shows you’ve improved! Keep reviewing to retain it.” This not only demonstrates the metric but also is a nice tutor feature (metacognitive feedback to the learner).
7. System Architecture Diagram


Figure: System architecture for the ontology-driven AI tutor. The diagram illustrates the flow: The user interacts with the Avatar Interface (which could be a chat UI). The interface passes the user’s query to the DeepPavlov Dialog Manager, which consults the Domain Ontology (for relevant knowledge) and prepares a prompt for Anthropic Claude (LLM). Claude generates a response (which might include an explanation or a question for the user) that is then returned to the user via the dialog manager. This loop continues, with the ontology providing a knowledge ground truth at each step, and the dialog manager guiding the session plan (possibly tracking the student’s progress). By integrating these components, the system ensures the AI tutor is both knowledgeable (via the ontology) and conversational (via Claude), wrapped in an interactive interface. This modular architecture aligns with best practices for Intelligent Tutoring Systems, where the expert knowledge (ontology) is separated from the teaching strategy (dialogue/LLM) and the interface​
EN.WIKIPEDIA.ORG
. (The diagram shows components such as “Ontology (Python/Owlready2)”, “LLM (Claude API)”, “Dialog Manager (DeepPavlov)”, and “User Interface (Avatar)” with arrows indicating data flow: e.g., user question -> DM -> ontology fetch + LLM prompt -> LLM -> answer -> DM -> user.)
8. Feasibility and Best Practices
This implementation plan is designed to be feasible for local deployment. All components (ontology, DeepPavlov agent, prompt logic) run on a local machine, and only API calls to Claude go out externally. This means with an internet connection for the LLM, one can run the tutor on a personal computer. We emphasize using lightweight or limited-size knowledge bases to keep memory and processing needs modest. DeepPavlov and Owlready2 are both capable of running on a standard laptop for a small ontology and low user volume. We also adhere to best practices for AI in education:
Accuracy and Consistency: By leveraging a vetted ontology and real datasets, we reduce the chance of the AI tutor giving incorrect information. The ontology acts as a single source of truth for key facts, and Claude is instructed to use it. This addresses the risk of LLM hallucination in educational content.
Pedagogical Soundness: The plan incorporates dialogue that is student-centered (asking questions, providing feedback). We follow proven strategies like active recall and incremental challenge, as seen in the design of quizzes and hints. The system can also easily incorporate multi-turn explanations and scaffolding – Claude can break down an answer into steps if the ontology indicates the student lacks a prerequisite concept.
User Engagement: Using an avatar/interface with possibly a friendly persona increases engagement. Even if just text-based for now, the conversational style is more engaging than static text. DeepPavlov’s framework allows adding emotional tone or even a voice interface later to make the tutor feel more “alive”.
Privacy and Local Control: Running locally (especially the knowledge base and dialog) means student interaction data isn’t by default sent to third parties (aside from the content of queries going to Claude API, which should be handled under appropriate privacy terms). For a POC, we can anonymize or avoid any personal data in prompts. In future, a self-hosted LLM could replace Claude for full privacy.
Scalability: While the POC is minimal, the architecture can scale. For example, one could expand the ontology to cover more subjects, swap Claude with a larger model or multiple models (Claude for general talk, perhaps a math solver tool for equations), and deploy the whole system on a server with multiple user support. The modular design (separating knowledge, logic, interface) makes such scaling manageable.
Finally, we plan to iterate on the POC based on testing. Any instance where Claude’s response is off or the dialogue feels unnatural will be analyzed and prompt tuning or ontology adjustment will be done. The combination of symbolic knowledge (ontology) and the neural conversational power (Claude) is a cutting-edge approach – our POC will provide insight into how effectively they complement each other in an educational context.
Conclusion
This structured implementation plan provides a clear path to build a minimal yet functional AI-driven tutor for a STEM domain. By integrating Anthropic Claude (for natural language dialogue) with a curated domain ontology (for factual grounding and lesson structure) and using DeepPavlov as the conversational orchestrator (and avatar interface), we harness the strengths of both symbolic and sub-symbolic AI. The step-by-step guide ensures we start from a solid knowledge foundation and incrementally add AI and interface capabilities. The resulting POC will demonstrate key features: it will answer students’ questions, guide them through topics, ask them questions to reinforce learning, and adapt its content based on an understanding of the domain structure. Crucially, we emphasize knowledge retention by building in quizzes and recall, aligning the system with educational best practices rather than just straight Q&A. If the POC succeeds, it will confirm the feasibility of a larger ontology-driven tutoring system. We would then look to expand the content, possibly integrate more student modeling (tracking which questions the student struggled with), and deploy on a platform where students can use it freely. The modular architecture diagrammed above can evolve into a full intelligent tutoring system, potentially offering personalized learning at scale. Through this POC, we aim to illustrate that combining a knowledge graph with a powerful LLM and a conversational agent can create an effective AI tutor – one that is explainable (ontology gives transparency to what it “knows”), interactive, and capable of improving learning outcomes (by focusing on engagement and retention). This minimal implementation will lay the groundwork for more advanced AI-augmented education tools in the near future. Sources:
Oguejiofor et al., “Intelligent tutoring systems: an ontology-based approach” – describes how ontologies provide formal, unambiguous knowledge representation for ITS​
ITC.SCIX.NET
.
Anthropic, “Introducing Claude” – Claude is a next-generation AI assistant accessible via API, capable of conversational tasks with high reliability​
ANTHROPIC.COM
.
DeepPavlov Team, DeepPavlov.ai – DeepPavlov is an open-source framework for chatbots/virtual assistants, featuring an Agent for multi-skill dialog management​
DEEPPAVLOV.AI
​
DEEPPAVLOV.AI
.
Owlready2 Documentation – Owlready2 allows Python manipulation of OWL ontologies, enabling import of ontology classes/instances as Python objects and even reasoning​
OWLREADY2.READTHEDOCS.IO
.
Celebal Technologies, “AI Tutor in Education” – Highlights measuring knowledge retention through integrated tests/quizzes in AI tutor systems​
CELEBALTECH.COM
.
DeepPavlov Dream (ACL 2023 Demo) – Demonstrates integrating LLMs into dialog systems with knowledge graphs for fact-checking and prompt-based control​
ACLANTHOLOGY.ORG
.
Wikipedia, “Cognitive Tutor” – Notes the standard four-component architecture of ITS (domain, student, tutor, interface) that our system design follows​
EN.WIKIPEDIA.ORG
.
Anthropic News, “Claude in education (Juni Tutor)” – Example of Claude being used in an online tutor bot, providing high-quality, detailed explanations for students​
ANTHROPIC.COM
.